## Тестовое задание Mindbox

Формулировка задания находится [здесь](/TASK.MD)

---

### Комментарии к заданию 1

Для задания 1 для примера использовал данные с kaggle - [ссылка](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)

Исходный csv-файл содержит 67.501979 млн. строк.

Датасет был отфильтрован по event_type = 'view', лишние колонки были удалены и переименованы.

Оставшиеся колонки: event_time (переименовано в timestamp), user_id (переименовано в customer_id), product_id

Осталось строк: 63.556110 млн

Подготовленный датасет выложил на диск - [ссылка](https://disk.yandex.ru/d/ALHAo0vrhgdr8Q)

Время обработки датасета  63 млн. строк на ноутбуке (AMD Ryzen 7 5800U, 16 gb оперативной памяти):
- медленным методом через df.apply - 469 секунд
- оптимизированным методом с использованием numba - 18 секунд
---

### Инструкция по запуску API из задания 2

1) Перед запуском необходимо создать и заполнить файл `.env`
   в корневой директории, пример заполнения приведен в файлике [.env.example](/.env.example)
2) Запустить контейнеры с базой и API командой:
   `docker compose up`.
3) Заходим в контейнер - `docker exec -it api bash`
4) Накатываем миграции - `alembic upgrade head`
5) Генерируем фейковые данные - `python utils/data_factories.py`
6) Для проверки работы сервиса нужно зайти по адресу - `http://localhost:8000/docs`
7) Тесты для API лежат в папке [task_2/tests](task_2/tests)



